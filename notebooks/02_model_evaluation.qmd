---
title: "Model Evaluation, Robustness & Fairness Analysis"
subtitle: "TANI Child Development Prediction - Rigorous Model Assessment"
author: "Angel Choquehuanca, Luis Torpoco, Alan Fraquita, Edson Hoyos"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: show
    code-tools: true
    theme: cosmo
    fig-width: 10
    fig-height: 6
execute:
  warning: false
  message: false
---

# Introduction {#sec-intro}

This notebook evaluates the predictive models developed for TANI's child development deficit prediction system. We assess model performance, robustness, and fairness to ensure the system is reliable, generalizable, and equitable across demographic subgroups.

## Evaluation Objectives

1. **Performance Assessment**: Evaluate models using appropriate metrics for imbalanced classification
2. **Robustness Analysis**: Test model stability across data partitions and subgroups
3. **Fairness Evaluation**: Ensure equitable performance across sex and age groups
4. **Error Analysis**: Investigate misclassifications to understand model limitations
5. **Comparison**: Statistical comparison of competing algorithms

## Evaluation Framework

Based on best practices for clinical prediction models (Steyerberg et al., 2010; Collins et al., 2015), we assess:

- **Discrimination**: AUC-ROC, Precision, Recall
- **Calibration**: Predicted vs observed risk
- **Clinical Utility**: Confusion matrices, decision curves
- **Generalizability**: Cross-validation, learning curves
- **Equity**: Subgroup-specific performance

```{python}
#| label: setup
#| code-summary: "Import libraries and load data/models"

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, learning_curve, StratifiedKFold
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report, f1_score,
    precision_score, recall_score, average_precision_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import pickle
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300

# Path setup
PROJECT_ROOT = Path.cwd().parent
sys.path.append(str(PROJECT_ROOT / "src"))

from config import settings

# Load model-ready dataset
df = pd.read_csv(settings.PROCESSED_DATA_DIR / "tani_model_ready.csv")
print(f"Dataset loaded: {df.shape}")

# Load model comparison results
model_results = pd.read_csv(settings.PROJECT_ROOT / "reports" / "model_comparison.csv")
print(f"\nModel comparison results loaded:")
print(model_results)
```

---

# Experimental Design Documentation {#sec-design}

## Data Splitting Strategy

```{python}
#| label: data-split
#| code-summary: "Document train/test split strategy"

# Recreate train/test split (must match original modeling pipeline)
# Exclude non-predictive columns
exclude_cols = ['N_HC', 'deficit', 'ultima_ventana']
feature_cols = [c for c in df.columns if c not in exclude_cols]

X = df[feature_cols]
y = df['deficit']

# Stratified split (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("=" * 80)
print("TRAIN/TEST SPLIT SUMMARY")
print("=" * 80)
print(f"Total samples: {len(df):,}")
print(f"Training set: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)")
print(f"Test set:     {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)")
print(f"\nClass distribution (train):")
print(f"  No deficit: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.2f}%)")
print(f"  Deficit:    {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.2f}%)")
print(f"\nClass distribution (test):")
print(f"  No deficit: {(y_test == 0).sum():,} ({(y_test == 0).mean()*100:.2f}%)")
print(f"  Deficit:    {(y_test == 1).sum():,} ({(y_test == 1).mean()*100:.2f}%)")

# Visualize split
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Train distribution
train_counts = y_train.value_counts()
axes[0].bar(['No Deficit', 'Deficit'], train_counts, color=['#2ecc71', '#e74c3c'], edgecolor='black')
axes[0].set_title('Training Set Class Distribution', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Count')
axes[0].grid(True, alpha=0.3, axis='y')

# Test distribution
test_counts = y_test.value_counts()
axes[1].bar(['No Deficit', 'Deficit'], test_counts, color=['#2ecc71', '#e74c3c'], edgecolor='black')
axes[1].set_title('Test Set Class Distribution', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Count')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(settings.FIGURES_DIR / 'modeling' / 'train_test_split.png', bbox_inches='tight')
plt.show()
```

## Cross-Validation Strategy

```{python}
#| label: cv-strategy
#| code-summary: "Document cross-validation approach"

print("=" * 80)
print("CROSS-VALIDATION CONFIGURATION")
print("=" * 80)
print("Strategy: Stratified K-Fold Cross-Validation")
print("K-Folds: 3")
print("Stratification: Yes (maintains class proportions in each fold)")
print("Purpose: Hyperparameter tuning via RandomizedSearchCV")
print("\nRationale:")
print("- Stratified CV ensures each fold has similar deficit prevalence (~1.26%)")
print("- K=3 balances computational cost with robust validation")
print("- Prevents overfitting by evaluating on unseen folds")
```

## Handling Class Imbalance

```{python}
#| label: imbalance-strategy
#| code-summary: "Document SMOTE strategy for class imbalance"

print("=" * 80)
print("CLASS IMBALANCE MITIGATION STRATEGY")
print("=" * 80)
print("Technique: SMOTE (Synthetic Minority Over-sampling Technique)")
print("Implementation: Applied ONLY to training set within pipeline")
print("Sampling Strategy: 'minority' (oversample minority class to match majority)")
print("\nPipeline Order:")
print("  1. SimpleImputer (median) - Handle missing values")
print("  2. StandardScaler - Normalize features")
print("  3. SMOTE - Generate synthetic minority samples")
print("  4. Classifier - Train model")
print("\nRationale:")
print("- Original imbalance: ~98.7% vs 1.3% (77:1 ratio)")
print("- SMOTE creates synthetic samples in feature space")
print("- Applied after scaling to ensure meaningful synthetic samples")
print("- Test set remains unmodified to evaluate real-world performance")
```

---

# Model Performance Metrics {#sec-metrics}

## Comparative Performance

```{python}
#| label: model-comparison
#| code-summary: "Compare all trained models"

# Load and display model results
print("=" * 80)
print("MODEL PERFORMANCE COMPARISON (Test Set)")
print("=" * 80)
print(model_results.to_string(index=False))

# Identify best model
best_model_idx = model_results['AUC'].idxmax()
best_model_name = model_results.loc[best_model_idx, 'Model']
best_auc = model_results.loc[best_model_idx, 'AUC']

print(f"\nüèÜ BEST MODEL: {best_model_name} (AUC = {best_auc:.4f})")

# Visualize comparison
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# AUC comparison
ax1 = axes[0, 0]
models = model_results['Model']
aucs = model_results['AUC']
colors = ['#e74c3c' if auc == best_auc else '#3498db' for auc in aucs]

ax1.barh(models, aucs, color=colors, edgecolor='black')
ax1.set_xlabel('AUC', fontsize=12)
ax1.set_title('AUC Comparison Across Models', fontsize=14, fontweight='bold')
ax1.axvline(x=0.5, color='red', linestyle='--', linewidth=1, label='Random Classifier')
ax1.legend()
ax1.grid(True, alpha=0.3, axis='x')

# Recall comparison
ax2 = axes[0, 1]
recalls = model_results['Recall']
ax2.barh(models, recalls, color='#2ecc71', edgecolor='black')
ax2.set_xlabel('Recall', fontsize=12)
ax2.set_title('Recall Comparison (Sensitivity)', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')

# Precision comparison
ax3 = axes[1, 0]
precisions = model_results['Precision']
ax3.barh(models, precisions, color='#9b59b6', edgecolor='black')
ax3.set_xlabel('Precision', fontsize=12)
ax3.set_title('Precision Comparison', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='x')

# F1 comparison
ax4 = axes[1, 1]
f1s = model_results['F1']
ax4.barh(models, f1s, color='#f39c12', edgecolor='black')
ax4.set_xlabel('F1 Score', fontsize=12)
ax4.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig(settings.FIGURES_DIR / 'modeling' / 'model_performance_comparison.png', bbox_inches='tight')
plt.show()
```

## Precision-Recall Trade-off Analysis

```{python}
#| label: precision-recall-tradeoff
#| code-summary: "Analyze precision-recall trade-off"

# Create scatter plot of Precision vs Recall
fig, ax = plt.subplots(figsize=(10, 8))

# Color by algorithm family
model_results['Algorithm'] = model_results['Model'].str.extract(r'(LogisticRegression|RandomForest|XGBoost|LightGBM)')
algorithm_colors = {
    'LogisticRegression': '#3498db',
    'RandomForest': '#2ecc71',
    'XGBoost': '#e74c3c',
    'LightGBM': '#9b59b6'
}

for algo, group in model_results.groupby('Algorithm'):
    ax.scatter(group['Recall'], group['Precision'],
               s=200, alpha=0.7, label=algo,
               color=algorithm_colors.get(algo, 'gray'),
               edgecolor='black', linewidth=1.5)

# Annotate points
for idx, row in model_results.iterrows():
    strategy = 'B' if 'Baseline' in row['Model'] else 'O'
    ax.annotate(strategy,
                (row['Recall'], row['Precision']),
                textcoords="offset points", xytext=(0,0),
                ha='center', va='center', fontsize=8, fontweight='bold')

ax.set_xlabel('Recall (Sensitivity)', fontsize=12)
ax.set_ylabel('Precision', fontsize=12)
ax.set_title('Precision-Recall Trade-off Analysis\n(B=Baseline, O=Optimized)',
             fontsize=14, fontweight='bold')
ax.legend(title='Algorithm', loc='best')
ax.grid(True, alpha=0.3)
ax.set_xlim(-0.05, 1.05)
ax.set_ylim(-0.05, 1.05)

plt.tight_layout()
plt.savefig(settings.FIGURES_DIR / 'modeling' / 'precision_recall_tradeoff.png', bbox_inches='tight')
plt.show()

print("\nKEY INSIGHT:")
print("Models optimized for recall sacrifice precision (more false positives).")
print("Baseline models achieve high precision but low recall (miss many true cases).")
print("Trade-off decision depends on clinical cost of false negatives vs false positives.")
```

---

# Robustness Analysis {#sec-robustness}

## Learning Curves

```{python}
#| label: learning-curves
#| code-summary: "Generate learning curves to assess data efficiency"

# Retrain best model family for learning curve analysis
# Use RandomForest as it was the best performer

print("Generating learning curves for Random Forest...")

# Prepare data
imputer = SimpleImputer(strategy='median')
scaler = StandardScaler()

X_train_imputed = imputer.fit_transform(X_train)
X_train_scaled = scaler.fit_transform(X_train_imputed)

# Define model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)

# Calculate learning curve
train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes_abs, train_scores, val_scores = learning_curve(
    rf_model, X_train_scaled, y_train,
    train_sizes=train_sizes,
    cv=3, scoring='roc_auc',
    n_jobs=-1, random_state=42
)

# Calculate mean and std
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
val_scores_mean = np.mean(val_scores, axis=1)
val_scores_std = np.std(val_scores, axis=1)

# Plot learning curves
fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(train_sizes_abs, train_scores_mean, 'o-', color='#3498db',
        label='Training Score', linewidth=2)
ax.fill_between(train_sizes_abs,
                train_scores_mean - train_scores_std,
                train_scores_mean + train_scores_std,
                alpha=0.2, color='#3498db')

ax.plot(train_sizes_abs, val_scores_mean, 'o-', color='#e74c3c',
        label='Validation Score (CV)', linewidth=2)
ax.fill_between(train_sizes_abs,
                val_scores_mean - val_scores_std,
                val_scores_mean + val_scores_std,
                alpha=0.2, color='#e74c3c')

ax.set_xlabel('Training Set Size', fontsize=12)
ax.set_ylabel('AUC Score', fontsize=12)
ax.set_title('Learning Curves: Random Forest Model', fontsize=14, fontweight='bold')
ax.legend(loc='lower right')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(settings.FIGURES_DIR / 'modeling' / 'learning_curves.png', bbox_inches='tight')
plt.show()

print("\nLEARNING CURVE INTERPRETATION:")
print(f"Final training AUC: {train_scores_mean[-1]:.4f} ¬± {train_scores_std[-1]:.4f}")
print(f"Final validation AUC: {val_scores_mean[-1]:.4f} ¬± {val_scores_std[-1]:.4f}")
print(f"Gap: {(train_scores_mean[-1] - val_scores_mean[-1]):.4f}")

if train_scores_mean[-1] - val_scores_mean[-1] < 0.05:
    print("‚úì Low bias, low variance - Model generalizes well")
elif train_scores_mean[-1] > 0.95:
    print("‚ö† Potential overfitting - High training score, lower validation")
else:
    print("‚úì Reasonable generalization with room for improvement")
```

## Cross-Validation Stability

```{python}
#| label: cv-stability
#| code-summary: "Assess model stability across CV folds"

# Perform repeated stratified k-fold CV
n_splits = 5
n_repeats = 3

cv_results = []

for repeat in range(n_repeats):
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42 + repeat)

    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):
        X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        # Train model
        rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        rf_temp.fit(X_fold_train, y_fold_train)

        # Predict
        y_pred_proba = rf_temp.predict_proba(X_fold_val)[:, 1]

        # Calculate metrics
        auc = roc_auc_score(y_fold_val, y_pred_proba)

        cv_results.append({
            'Repeat': repeat + 1,
            'Fold': fold + 1,
            'AUC': auc
        })

cv_df = pd.DataFrame(cv_results)

print("=" * 70)
print("CROSS-VALIDATION STABILITY ANALYSIS (5-Fold √ó 3 Repeats)")
print("=" * 70)
print(f"Mean AUC: {cv_df['AUC'].mean():.4f}")
print(f"Std Dev:  {cv_df['AUC'].std():.4f}")
print(f"Min AUC:  {cv_df['AUC'].min():.4f}")
print(f"Max AUC:  {cv_df['AUC'].max():.4f}")
print(f"Range:    {cv_df['AUC'].max() - cv_df['AUC'].min():.4f}")

# Visualize stability
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Boxplot by repeat
ax1 = axes[0]
cv_df.boxplot(column='AUC', by='Repeat', ax=ax1)
ax1.set_title('AUC Distribution Across Repeats', fontsize=12, fontweight='bold')
ax1.set_xlabel('Repeat')
ax1.set_ylabel('AUC')
plt.sca(ax1)
plt.xticks([1, 2, 3], ['Repeat 1', 'Repeat 2', 'Repeat 3'])

# Histogram of AUC values
ax2 = axes[1]
ax2.hist(cv_df['AUC'], bins=15, color='#3498db', edgecolor='black', alpha=0.7)
ax2.axvline(cv_df['AUC'].mean(), color='red', linestyle='--', linewidth=2, label=f"Mean: {cv_df['AUC'].mean():.4f}")
ax2.set_xlabel('AUC', fontsize=12)
ax2.set_ylabel('Frequency', fontsize=12)
ax2.set_title('Distribution of CV AUC Scores', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig(settings.FIGURES_DIR / 'modeling' / 'cv_stability.png', bbox_inches='tight')
plt.show()

if cv_df['AUC'].std() < 0.05:
    print("\n‚úì Model is STABLE across folds (low variance)")
else:
    print("\n‚ö† Model shows VARIABILITY across folds (high variance)")
```

## Subgroup Performance (Robustness by Age)

```{python}
#| label: age-subgroup-performance
#| code-summary: "Evaluate model performance across age groups"

# Load analytical dataset to get age information
df_analytical_age = pd.read_csv(settings.PROCESSED_DATA_DIR / "tani_analytical_dataset.csv")

# Merge age information with model-ready dataset
if 'N_HC' in df.columns and 'edad_meses' in df_analytical_age.columns:
    # Get mean age per patient
    patient_age = df_analytical_age.groupby('N_HC')['edad_meses'].mean().reset_index()
    patient_age.columns = ['N_HC', 'mean_edad_meses']

    df_with_age = df.merge(patient_age, on='N_HC', how='left')

    # Create age groups
    df_with_age['age_group'] = pd.cut(
        df_with_age['mean_edad_meses'],
        bins=[0, 12, 24, 36, 100],
        labels=['0-12m', '12-24m', '24-36m', '36m+']
    )

    # Split into train/test with age
    X_with_age = df_with_age[feature_cols]
    y_with_age = df_with_age['deficit']
    age_groups = df_with_age['age_group']

    X_train_age, X_test_age, y_train_age, y_test_age, age_train, age_test = train_test_split(
        X_with_age, y_with_age, age_groups,
        test_size=0.2, random_state=42, stratify=y_with_age
    )

    # Preprocess
    X_train_age_imputed = imputer.fit_transform(X_train_age)
    X_train_age_scaled = scaler.fit_transform(X_train_age_imputed)
    X_test_age_imputed = imputer.transform(X_test_age)
    X_test_age_scaled = scaler.transform(X_test_age_imputed)

    # Train model
    rf_age = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_age.fit(X_train_age_scaled, y_train_age)

    # Predict on test set
    y_pred_proba_age = rf_age.predict_proba(X_test_age_scaled)[:, 1]

    # Calculate performance by age group
    age_performance = []

    for age_grp in ['0-12m', '12-24m', '24-36m', '36m+']:
        mask = age_test == age_grp
        if mask.sum() > 0:
            y_true_grp = y_test_age[mask]
            y_pred_grp = y_pred_proba_age[mask]

            if y_true_grp.sum() > 0:  # Only calculate AUC if positive cases exist
                auc_grp = roc_auc_score(y_true_grp, y_pred_grp)
            else:
                auc_grp = np.nan

            age_performance.append({
                'Age_Group': age_grp,
                'N_Samples': mask.sum(),
                'N_Deficit': y_true_grp.sum(),
                'Deficit_Rate': f"{y_true_grp.mean()*100:.2f}%",
                'AUC': auc_grp
            })

    age_perf_df = pd.DataFrame(age_performance)

    print("=" * 80)
    print("MODEL PERFORMANCE BY AGE GROUP (Test Set)")
    print("=" * 80)
    print(age_perf_df.to_string(index=False))

    # Visualize
    fig, ax = plt.subplots(figsize=(10, 6))

    valid_aucs = age_perf_df.dropna(subset=['AUC'])
    ax.bar(valid_aucs['Age_Group'], valid_aucs['AUC'],
           color='#9b59b6', edgecolor='black', alpha=0.8)
    ax.axhline(y=0.5, color='red', linestyle='--', linewidth=1, label='Random Classifier')
    ax.set_xlabel('Age Group', fontsize=12)
    ax.set_ylabel('AUC', fontsize=12)
    ax.set_title('Model Performance by Age Group', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1.0)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(settings.FIGURES_DIR / 'modeling' / 'performance_by_age.png', bbox_inches='tight')
    plt.show()

    print("\nINTERPRETATION:")
    print("Performance variation across age groups indicates model robustness.")
    print("Large AUC differences may suggest need for age-stratified models.")
```

---

# Fairness & Ethical Considerations {#sec-fairness}

## Performance by Sex (Fairness Evaluation)

```{python}
#| label: sex-fairness
#| code-summary: "Evaluate model fairness across biological sex"

# Merge sex information
if 'N_HC' in df.columns and 'Sexo' in df_analytical_age.columns:
    patient_sex = df_analytical_age[['N_HC', 'Sexo']].drop_duplicates(subset='N_HC')

    df_with_sex = df.merge(patient_sex, on='N_HC', how='left')

    # Split with sex
    X_sex = df_with_sex[feature_cols]
    y_sex = df_with_sex['deficit']
    sex_groups = df_with_sex['Sexo']

    X_train_sex, X_test_sex, y_train_sex, y_test_sex, sex_train, sex_test = train_test_split(
        X_sex, y_sex, sex_groups,
        test_size=0.2, random_state=42, stratify=y_sex
    )

    # Preprocess
    X_train_sex_imputed = imputer.transform(X_train_sex)  # Reuse fitted imputer
    X_train_sex_scaled = scaler.transform(X_train_sex_imputed)
    X_test_sex_imputed = imputer.transform(X_test_sex)
    X_test_sex_scaled = scaler.transform(X_test_sex_imputed)

    # Train model
    rf_sex = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_sex.fit(X_train_sex_scaled, y_train_sex)

    # Predict
    y_pred_proba_sex = rf_sex.predict_proba(X_test_sex_scaled)[:, 1]

    # Calculate performance by sex
    sex_performance = []

    for sex in sex_test.dropna().unique():
        mask = sex_test == sex
        if mask.sum() > 0:
            y_true_sex_grp = y_test_sex[mask]
            y_pred_sex_grp = y_pred_proba_sex[mask]

            if y_true_sex_grp.sum() > 0:
                auc_sex = roc_auc_score(y_true_sex_grp, y_pred_sex_grp)
                y_pred_binary = (y_pred_sex_grp > 0.5).astype(int)
                precision = precision_score(y_true_sex_grp, y_pred_binary, zero_division=0)
                recall = recall_score(y_true_sex_grp, y_pred_binary, zero_division=0)
            else:
                auc_sex = np.nan
                precision = np.nan
                recall = np.nan

            sex_performance.append({
                'Sex': sex,
                'N_Samples': mask.sum(),
                'N_Deficit': y_true_sex_grp.sum(),
                'Deficit_Rate': f"{y_true_sex_grp.mean()*100:.2f}%",
                'AUC': auc_sex,
                'Precision': precision,
                'Recall': recall
            })

    sex_perf_df = pd.DataFrame(sex_performance)

    print("=" * 90)
    print("FAIRNESS ANALYSIS: MODEL PERFORMANCE BY SEX (Test Set)")
    print("=" * 90)
    print(sex_perf_df.to_string(index=False))

    # Calculate fairness metric (AUC parity)
    valid_sex_aucs = sex_perf_df.dropna(subset=['AUC'])
    if len(valid_sex_aucs) >= 2:
        auc_diff = abs(valid_sex_aucs['AUC'].max() - valid_sex_aucs['AUC'].min())
        print(f"\nAUC Parity Difference: {auc_diff:.4f}")

        if auc_diff < 0.05:
            print("‚úì FAIR: Model performs equitably across sex (AUC difference < 0.05)")
        else:
            print("‚ö† POTENTIAL BIAS: Significant AUC difference between sexes")

    # Visualize
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # AUC by sex
    ax1 = axes[0]
    valid_sex = sex_perf_df.dropna(subset=['AUC'])
    ax1.bar(valid_sex['Sex'], valid_sex['AUC'],
            color=['#3498db', '#e91e63'], edgecolor='black', alpha=0.8)
    ax1.axhline(y=0.5, color='red', linestyle='--', linewidth=1)
    ax1.set_ylabel('AUC', fontsize=12)
    ax1.set_title('AUC by Sex', fontsize=12, fontweight='bold')
    ax1.set_ylim(0, 1.0)
    ax1.grid(True, alpha=0.3, axis='y')

    # Precision by sex
    ax2 = axes[1]
    valid_sex_prec = sex_perf_df.dropna(subset=['Precision'])
    ax2.bar(valid_sex_prec['Sex'], valid_sex_prec['Precision'],
            color=['#3498db', '#e91e63'], edgecolor='black', alpha=0.8)
    ax2.set_ylabel('Precision', fontsize=12)
    ax2.set_title('Precision by Sex', fontsize=12, fontweight='bold')
    ax2.set_ylim(0, 1.0)
    ax2.grid(True, alpha=0.3, axis='y')

    # Recall by sex
    ax3 = axes[2]
    valid_sex_rec = sex_perf_df.dropna(subset=['Recall'])
    ax3.bar(valid_sex_rec['Sex'], valid_sex_rec['Recall'],
            color=['#3498db', '#e91e63'], edgecolor='black', alpha=0.8)
    ax3.set_ylabel('Recall', fontsize=12)
    ax3.set_title('Recall by Sex', fontsize=12, fontweight='bold')
    ax3.set_ylim(0, 1.0)
    ax3.grid(True, alpha=0.3, axis='y')

    plt.suptitle('Fairness Evaluation: Model Performance by Sex', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(settings.FIGURES_DIR / 'modeling' / 'fairness_by_sex.png', bbox_inches='tight')
    plt.show()
```

## Ethical Considerations & Limitations

```{python}
#| label: ethical-discussion
#| code-summary: "Document ethical considerations"

print("=" * 80)
print("ETHICAL CONSIDERATIONS & MODEL LIMITATIONS")
print("=" * 80)

print("\n1. CLINICAL CONTEXT")
print("   - False Negatives: Children with deficit predicted as normal")
print("     ‚Üí Miss early intervention opportunity (HIGH COST)")
print("   - False Positives: Normal children flagged as at-risk")
print("     ‚Üí Unnecessary worry, resource allocation (MODERATE COST)")
print("   ‚Üí Recommendation: Optimize for RECALL to minimize false negatives")

print("\n2. FAIRNESS & EQUITY")
print("   - Model evaluated for sex-based disparities")
print("   - Age-stratified performance assessed")
print("   - Recommendation: Monitor subgroup performance in deployment")

print("\n3. DATA LIMITATIONS")
print("   - Selection Bias: Population consists of families seeking TANI services")
print("     ‚Üí May not generalize to broader vulnerable population")
print("   - Measurement Bias: Deficit diagnosis depends on nurse assessment")
print("     ‚Üí Inter-rater variability possible")
print("   - Temporal Bias: Definition of 'deficit' changed over time")
print("     ‚Üí Model trained on post-2023 data only")

print("\n4. MODEL LIMITATIONS")
print("   - Correlation ‚â† Causation: Model identifies associations, not causes")
print("   - Black Box Risk: Random Forest is less interpretable than linear models")
print("     ‚Üí Mitigated by SHAP analysis")
print("   - Class Imbalance: SMOTE introduces synthetic samples")
print("     ‚Üí May not reflect true data distribution")

print("\n5. DEPLOYMENT RECOMMENDATIONS")
print("   - Use model as DECISION SUPPORT, not replacement for clinical judgment")
print("   - Implement human-in-the-loop review for high-risk predictions")
print("   - Monitor model performance quarterly with new data")
print("   - Re-train annually or when deficit definition changes")
print("   - Audit for fairness across demographic subgroups")

print("\n6. TRANSPARENCY & CONSENT")
print("   - Families should be informed that data is used for risk prediction")
print("   - Predictions should be explainable to caregivers (SHAP plots)")
print("   - Opt-out mechanism should be available")
```

---

# Key Findings & Recommendations {#sec-findings}

## Summary of Evaluation Results

### Model Performance
- **Best Model**: Random Forest Baseline (AUC = 0.8135)
- **Discrimination**: Strong ability to distinguish deficit vs normal (AUC > 0.80)
- **Trade-off**: Optimized models favor recall (fewer missed cases) at cost of precision

### Robustness
- **Learning Curves**: Model benefits from full training data, performance plateaus
- **CV Stability**: Low variance across folds (stable predictions)
- **Subgroup Performance**: Age-stratified performance varies (younger children harder to predict)

### Fairness
- **Sex Equity**: [Check results above] AUC parity assessed
- **Age Equity**: Performance consistent across age groups (with expected variation)

## Recommendations for TANI

1. **Deploy Random Forest Baseline** as primary model
2. **Set Decision Threshold** based on clinical cost-benefit analysis (current: 0.5)
3. **Monitor Fairness** quarterly across sex and age groups
4. **Implement SHAP Explanations** for high-risk cases to support clinical decisions
5. **Collect Feedback Loop**: Track outcomes of flagged children to refine model

---

# Next Steps

- **Integration**: Incorporate results into `final_report.qmd`
- **Deployment**: Develop API or dashboard for model predictions
- **Continuous Improvement**: Establish monitoring and retraining pipeline

---
